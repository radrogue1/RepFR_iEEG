{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d8e422-4624-44b5-a117-abd481853592",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/ptsa/data/readers/__init__.py:19: FutureWarning: PTSA readers will be removed in a future release. Please consider using the cmlreaders package instead: https://github.com/pennmem/cmlreaders\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions ...  imports imported <3 \n",
      "functions imported\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created by Brandon Katerman on March 13th, 2022\n",
    "\n",
    "Last Modified: 04/17/22 by Ricardo Adrogue\n",
    "\n",
    "Current stats to run on encoding events for RepFR1 -- successor to sme_tstat.py\n",
    "'''\n",
    "\n",
    "print(\"loading modules\")\n",
    "from time import time\n",
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import scipy.spatial as spatial\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import cmlreaders as cml\n",
    "from cmlreaders import CMLReader, get_data_index \n",
    "\n",
    "import ptsa \n",
    "#from ptsa.data.TimeSeriesX import TimeSeries\n",
    "from ptsa.data.timeseries import TimeSeries\n",
    "from ptsa.data import timeseries\n",
    "from ptsa.data.readers import BaseEventReader\n",
    "\n",
    "from ptsa.data.filters import MorletWaveletFilter\n",
    "from ptsa.data.filters import ButterworthFilter\n",
    "\n",
    "print(\"functions ... \", end = ' ')\n",
    "# imports all functions needed for this to work\n",
    "from run_matched_deliberations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04712e7-7386-4428-8c63-b7cf853a9a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/cmlreaders/readers/electrodes.py:277: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  json_normalize(flat_contact_data).set_index('name'),\n",
      "/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/cmlreaders/readers/electrodes.py:278: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  json_normalize(flat_pairs_data).set_index('names')\n"
     ]
    }
   ],
   "source": [
    "# makes a list of RepFR1 subjects with electrodes in ROIs\n",
    "data = cml.get_data_index(kind = 'r1'); data = data[data['experiment'] == 'RepFR1']\n",
    "# pulls all contacts from the montage\n",
    "loc = []\n",
    "for subject, df in data.groupby('subject'):\n",
    "    for session in pd.unique(df['session']):\n",
    "        r = cml.CMLReader(subject=subject, experiment='RepFR1', session=session)\n",
    "        temp = r.load('localization')\n",
    "        temp['subject'] = pd.Series(subject, index=temp.index)\n",
    "        temp['session'] = pd.Series(session, index=temp.index)\n",
    "        loc.append(temp)\n",
    "all_loc = pd.concat(loc)\n",
    "all_loc_p = all_loc.loc['pairs']\n",
    "# loc_p[loc_p['atlases.dk'].]\n",
    "# all_loc_p['atlases.whole_brain'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b23fd07-0ffb-4bcf-816e-857232da5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describes regions of interest\n",
    "regions = ['parietal', 'Hippocampus', 'entorhinal', 'Amygdala','parietal', \n",
    "           'parahippocampal', 'frontal gyrus', 'inferior frontal gyrus', 'middle frontal gyrus',\n",
    "          'superior frontal gyrus', 'temporal gyrus', 'MTG', \n",
    "           'inferior temporal gyrus', 'superior temporal gyrus', 'MTL']\n",
    "hemispheres = ['Right', 'Left', '']\n",
    "# stores subjects by ROI in a dataframe\n",
    "results = pd.DataFrame(columns=['region', 'num_subs', 'subjects'])\n",
    "for h in hemispheres:\n",
    "    for r in regions:\n",
    "        \n",
    "        subs = all_loc_p[(all_loc_p['atlases.whole_brain'].str.contains(r)) & all_loc_p['atlases.whole_brain'].\n",
    "                     str.contains(h)].subject.unique()\n",
    "        n = all_loc_p[(all_loc_p['atlases.whole_brain'].str.contains(r)) & all_loc_p['atlases.whole_brain'].\n",
    "                     str.contains(h)].subject.nunique()\n",
    "        if h == '':\n",
    "            results = results.append(pd.DataFrame(dict(region = r, num_subs = n, subjects = [subs]), index = [len(results)]))\n",
    "        else:\n",
    "            results = results.append(pd.DataFrame(dict(region = h +' ' + r, num_subs = n, subjects = [subs]), index = [len(results)]))\n",
    "# LTL = results[(results.hemisphere == 'Left') & (results.region == 'temporal')]\n",
    "\n",
    "# display(results.set_index('region'))\n",
    "\n",
    "\n",
    "# path = '/scratch/radrogue/RepFR1/'\n",
    "# print('processed regions:')\n",
    "# for file in os.listdir(path):\n",
    "#     d = os.path.join(path, file)\n",
    "#     if os.path.isdir(d):\n",
    "#         words = d.split('/')[-1].split('_')\n",
    "#         print(words[0] + ' ' + words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2094fa67-2d06-45db-b95e-a0e7de59977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 with electrodes in localization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/cmlreaders/readers/electrodes.py:277: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  json_normalize(flat_contact_data).set_index('name'),\n",
      "/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/cmlreaders/readers/electrodes.py:278: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  json_normalize(flat_pairs_data).set_index('names')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 with region in localization & pairs\n",
      "['R1501J' 'R1514E' 'R1516E' 'R1528E' 'R1534D' 'R1566D' 'R1582E' 'R1587J'\n",
      " 'R1604J']\n"
     ]
    }
   ],
   "source": [
    "# set your hemisphere and region here\n",
    "hemisphere = 'Right'\n",
    "region = 'MTL'\n",
    "\n",
    "# selects the subjects with electrodes in your selected region\n",
    "# MTL is multiple regions, so specifically have to look through this way\n",
    "if region == 'MTL':\n",
    "    subs = results[results.region == hemisphere+' '+'parahippocampal'].subjects.iloc[0]\n",
    "    subs = np.concatenate([subs, results[results.region == hemisphere+' '+'Amygdala'].subjects.iloc[0]])\n",
    "    subs = np.concatenate([subs, results[results.region == hemisphere+' '+'entorhinal'].subjects.iloc[0]])\n",
    "    subs = np.unique(subs)\n",
    "else:\n",
    "    subs = subs = results[results.region == hemisphere+' '+region].subjects.iloc[0]\n",
    "print(len(subs), 'with electrodes in localization')\n",
    "\n",
    "\n",
    "# checks that the pairs in that region were actually recorded from\n",
    "# We only record 128 channels for most of this data\n",
    "# Localization includes all electrodes (up to 256)\n",
    "# So this checks that the electrodes are also in pairs, which only shows pairs where\n",
    "# data was recorded\n",
    "pairs = []\n",
    "for sub in subs:\n",
    "    data = get_data_index('r1'); data = data[(data.experiment == 'RepFR1') & (data.subject==sub)]\n",
    "    r = cml.CMLReader(subject=sub, experiment='RepFR1', session = data.session.iloc[0])\n",
    "    loc = r.load(\"localization\")\n",
    "    t_pairs = r.load('pairs')\n",
    "    loc_p = loc.loc['pairs']\n",
    "    if region == 'MTL':\n",
    "        f_loc_p = loc_p[(loc_p['atlases.whole_brain'].str.contains(hemisphere)) & \n",
    "                        ((loc_p['atlases.whole_brain'].str.contains('parahippocampal')) | (loc_p['atlases.whole_brain'].str.contains('Amygdala')) \n",
    "                         | (loc_p['atlases.whole_brain'].str.contains('entorhinal')))]\n",
    "    else:\n",
    "        f_loc_p = loc_p[(loc_p['atlases.whole_brain'].str.contains(hemisphere)) & loc_p['atlases.whole_brain'].str.contains(region)]\n",
    "    pairs_filter = []\n",
    "    for labels in f_loc_p.index:\n",
    "        biploar_label = labels[0]+'-'+labels[1]\n",
    "        pairs_filter.append(biploar_label)\n",
    "    t_pairs = t_pairs[t_pairs.label.isin(pairs_filter)]\n",
    "    if t_pairs.empty:\n",
    "        subs = subs[subs != sub]\n",
    "    else:\n",
    "        pairs.append(t_pairs)\n",
    "print(len(subs), 'with region in localization & pairs')\n",
    "# print(subs)\n",
    "# makes lists of hemi and reg as same length as subs array\n",
    "# this is because Dask requires all of your parameters to have the same shape\n",
    "hemispheres = []\n",
    "regions = []\n",
    "print(subs)\n",
    "for i in subs:\n",
    "    regions.append(region)\n",
    "    hemispheres.append(hemisphere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe8b363-baf1-41ab-8d6b-7820312deec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import Dask and Dask functions to run script on the cluster\n",
    "import CMLDask\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "from dask import config\n",
    "config.set({'timeouts':{'connect':'90s', 'tcp':'120s'}})\n",
    "try: client.shutdown()\n",
    "except: print('no client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93f73c6-ed91-46c0-9d1a-99158af7a280",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for radrogue is 51417\n",
      "{'dashboard_address': ':51417'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN radrogue@rhino2.psych.upenn.edu -L 8000:192.168.86.145:51417` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to localhost:8000 in your browser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=CommClosedError('in <TCP (closed) Scheduler Broadcast local=tcp://192.168.86.145:54756 remote=tcp://192.168.86.135:40950>: Stream is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/global/Anaconda/2019-10/lib/python3.7/site-packages/tornado/gen.py\", line 742, in run\n",
      "    yielded = self.gen.throw(*exc_info)  # type: ignore\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 221, in quiet\n",
      "    yield task\n",
      "  File \"/usr/global/Anaconda/2019-10/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/scheduler.py\", line 6015, in send_message\n",
      "    resp = await send_recv(comm, close=True, serializers=serializers, **msg)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 663, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler Broadcast local=tcp://192.168.86.145:54756 remote=tcp://192.168.86.135:40950>: Stream is closed\n",
      "Exception in thread WorkerMemory:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 205, in read\n",
      "    frames_nbytes = await stream.read_bytes(fmt_size)\n",
      "tornado.iostream.StreamClosedError: Stream is closed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/dask_memusage.py\", line 66, in _fetch_memory\n",
      "    worker_to_mem = client.run(_process_memory)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/client.py\", line 2472, in run\n",
      "    return self.sync(self._run, function, *args, **kwargs)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 311, in sync\n",
      "    self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 364, in sync\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 349, in f\n",
      "    result[0] = yield future\n",
      "  File \"/usr/global/Anaconda/2019-10/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/client.py\", line 2401, in _run\n",
      "    nanny=nanny,\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 886, in send_recv_from_rpc\n",
      "    result = await send_recv(comm=comm, op=key, **kwargs)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 663, in send_recv\n",
      "    response = await comm.read(deserializers=deserializers)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 221, in read\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 128, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.broadcast local=tcp://192.168.86.145:39204 remote=tcp://192.168.86.145:45989>: Stream is closed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['R1501J_0 worked :)', 'R1501J_1 worked :)', 'R1501J_2 worked :)'],\n",
       " ['R1514E_0 worked :)'],\n",
       " ['R1516E_0 worked :)', 'R1516E_1 worked :)'],\n",
       " ['R1528E_0 worked :)', 'R1528E_1 worked :)'],\n",
       " ['R1534D_0 worked :)'],\n",
       " ['R1566D_1 worked :)', 'R1566D_3 worked :)', 'R1566D_4 worked :)'],\n",
       " ['R1582E_0 worked :)', 'R1582E_1 worked :)', 'R1582E_2 worked :)'],\n",
       " ['R1587J_1 worked :)', 'R1587J_2 worked :)', 'R1587J_3 worked :)'],\n",
       " ['R1604J_0 worked :)']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates cluster jobs, 1 for each subject, each with 10 GB limit of\n",
    "# memory to calculate powers for subject and region\n",
    "# client.map(function, p1, p2, p3)\n",
    "\n",
    "client = CMLDask.new_dask_client(\"iEEG_powers\", \"10GB\")\n",
    "futures = client.map(get_rec_powers, subs, pairs, hemispheres, regions)\n",
    "# waits until the cluster job is complete\n",
    "wait(futures)\n",
    "# gathers any errors\n",
    "power_errors = client.gather(futures)\n",
    "# shuts down the cluster\n",
    "client.shutdown()\n",
    "# displays errors\n",
    "power_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ebf5456-47d2-4ab9-92e0-9fdb4f2c8ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for radrogue is 51417\n",
      "{'dashboard_address': ':51417'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN radrogue@rhino2.psych.upenn.edu -L 8000:192.168.86.145:51417` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to localhost:8000 in your browser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['R1501J <3',\n",
       " 'R1514E <3',\n",
       " 'R1516E <3',\n",
       " 'R1528E <3',\n",
       " 'R1534D <3',\n",
       " 'R1566D failed :( all the input array dimensions for the concatenation axis must match exactly, but along dimension 3, the array at index 0 has size 590 and the array at index 1 has size 2356',\n",
       " 'R1582E <3',\n",
       " 'R1587J <3',\n",
       " 'R1604J <3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread WorkerMemory:\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/core.py\", line 286, in connect\n",
      "    timeout=min(intermediate_cap, time_left()),\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/asyncio/tasks.py\", line 442, in wait_for\n",
      "    return fut.result()\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 410, in connect\n",
      "    convert_stream_closed_error(self, e)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/tcp.py\", line 126, in convert_stream_closed_error\n",
      "    raise CommClosedError(f\"in {obj}: {exc.__class__.__name__}: {exc}\") from exc\n",
      "distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x2b1a738a5a90>: ConnectionRefusedError: [Errno 111] Connection refused\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home1/radrogue/.conda/envs/environmentname/lib/python3.7/site-packages/dask_memusage.py\", line 66, in _fetch_memory\n",
      "    worker_to_mem = client.run(_process_memory)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/client.py\", line 2472, in run\n",
      "    return self.sync(self._run, function, *args, **kwargs)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 311, in sync\n",
      "    self.loop, func, *args, callback_timeout=callback_timeout, **kwargs\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 364, in sync\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/utils.py\", line 349, in f\n",
      "    result[0] = yield future\n",
      "  File \"/usr/global/Anaconda/2019-10/lib/python3.7/site-packages/tornado/gen.py\", line 735, in run\n",
      "    value = future.result()\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/client.py\", line 2401, in _run\n",
      "    nanny=nanny,\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 883, in send_recv_from_rpc\n",
      "    comm = await self.pool.connect(self.addr)\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 1071, in connect\n",
      "    raise exc\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/core.py\", line 1055, in connect\n",
      "    comm = await fut\n",
      "  File \"/home1/radrogue/.local/lib/python3.7/site-packages/distributed/comm/core.py\", line 310, in connect\n",
      "    ) from active_exception\n",
      "OSError: Timed out trying to connect to tcp://192.168.86.145:39332 after 30 s\n",
      "\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "# creates new cluster jobs, 1 for each subject, 50GB memory to calculate t-stats\n",
    "client = CMLDask.new_dask_client(\"iEEG_stats\", \"25GB\")\n",
    "futures = client.map(rec_power_statistics, subs, pairs, hemispheres, regions)\n",
    "\n",
    "# gathers report on how this function ran\n",
    "# good means it was completed, otherwise shows error message\n",
    "wait(futures)\n",
    "ahh = client.gather(futures)\n",
    "\n",
    "# shuts down client\n",
    "client.shutdown()\n",
    "ahh"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36d62401-0a43-436c-b7cc-c89663f525dc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "# import Dask and Dask functions to run script on the cluster\n",
    "import CMLDask\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "from dask import config\n",
    "\n",
    "best_regions = ['Left_MTG', 'Right_MTG', 'Left_frontal gyrus', 'Right_frontal gyrus',\n",
    "                'Left_Hippocampus','Right_Hippocampus', 'Left_MTL', 'Right_MTL']\n",
    "for r in best_regions:\n",
    "    print(r)\n",
    "    hemisphere = r.split('_')[0]\n",
    "    region = r.split('_')[1]\n",
    "    \n",
    "    # selects the subjects with electrodes in your selected region\n",
    "    # MTL is multiple regions, so specifically have to look through this way\n",
    "    if region == 'MTL':\n",
    "        subs = results[results.region == hemisphere+' '+'parahippocampal'].subjects.iloc[0]\n",
    "        subs = np.concatenate([subs, results[results.region == hemisphere+' '+'Amygdala'].subjects.iloc[0]])\n",
    "        subs = np.concatenate([subs, results[results.region == hemisphere+' '+'entorhinal'].subjects.iloc[0]])\n",
    "        subs = np.unique(subs)\n",
    "    else:\n",
    "        subs = subs = results[results.region == hemisphere+' '+region].subjects.iloc[0]\n",
    "    print(len(subs), 'with electrodes in localization')\n",
    "\n",
    "\n",
    "    # checks that the pairs in that region were actually recorded from\n",
    "    # We only record 128 channels for most of this data\n",
    "    # Localization includes all electrodes (up to 256)\n",
    "    # So this checks that the electrodes are also in pairs, which only shows pairs where\n",
    "    # data was recorded\n",
    "    pairs = []\n",
    "    for sub in subs:\n",
    "        data = get_data_index('r1'); data = data[(data.experiment == 'RepFR1') & (data.subject==sub)]\n",
    "        r = cml.CMLReader(subject=sub, experiment='RepFR1', session = data.session.iloc[0])\n",
    "        loc = r.load(\"localization\")\n",
    "        t_pairs = r.load('pairs')\n",
    "        loc_p = loc.loc['pairs']\n",
    "        if region == 'MTL':\n",
    "            f_loc_p = loc_p[(loc_p['atlases.whole_brain'].str.contains(hemisphere)) & \n",
    "                            ((loc_p['atlases.whole_brain'].str.contains('parahippocampal')) | (loc_p['atlases.whole_brain'].str.contains('Amygdala')) \n",
    "                             | (loc_p['atlases.whole_brain'].str.contains('entorhinal')))]\n",
    "        else:\n",
    "            f_loc_p = loc_p[(loc_p['atlases.whole_brain'].str.contains(hemisphere)) & loc_p['atlases.whole_brain'].str.contains(region)]\n",
    "        pairs_filter = []\n",
    "        for labels in f_loc_p.index:\n",
    "            biploar_label = labels[0]+'-'+labels[1]\n",
    "            pairs_filter.append(biploar_label)\n",
    "        t_pairs = t_pairs[t_pairs.label.isin(pairs_filter)]\n",
    "        if t_pairs.empty:\n",
    "            subs = subs[subs != sub]\n",
    "        else:\n",
    "            pairs.append(t_pairs)\n",
    "    print(len(subs), 'with region in localization & pairs')\n",
    "    # print(subs)\n",
    "    # makes lists of hemi and reg as same length as subs array\n",
    "    # this is because Dask requires all of your parameters to have the same shape\n",
    "    hemispheres = []\n",
    "    regions = []\n",
    "    print(subs)\n",
    "    for i in subs:\n",
    "        regions.append(region)\n",
    "        hemispheres.append(hemisphere)\n",
    "    try: client.shutdown()\n",
    "    except: print('no client')\n",
    "    \n",
    "    client = CMLDask.new_dask_client(\"iEEG_powers\", \"10GB\")\n",
    "    futures = client.map(get_rec_powers, subs, pairs, hemispheres, regions)\n",
    "    # waits until the cluster job is complete\n",
    "    wait(futures)\n",
    "    # gathers any errors\n",
    "    power_errors = client.gather(futures)\n",
    "    # shuts down the cluster\n",
    "    client.shutdown()\n",
    "    # displays errors\n",
    "    display(power_errors)\n",
    "    # creates new cluster jobs, 1 for each subject, 50GB memory to calculate t-stats\n",
    "    client = CMLDask.new_dask_client(\"iEEG_stats\", \"25GB\")\n",
    "    futures = client.map(rec_power_statistics, subs, pairs, hemispheres, regions)\n",
    "\n",
    "    # gathers report on how this function ran\n",
    "    # good means it was completed, otherwise shows error message\n",
    "    wait(futures)\n",
    "    ahh = client.gather(futures)\n",
    "\n",
    "    # shuts down client\n",
    "    client.shutdown()\n",
    "    display(ahh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environmentname",
   "language": "python",
   "name": "environmentname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
